# MAVRL
MAVRL: Learn to Fly in Cluttered Environments with Varying Speed

# 1. Introduction
Many existing obstacle avoidance algorithms overlook the crucial balance between safety and agility, especially in environments of varying complexity. In our study, we introduce an obstacle avoidance pipeline based on reinforcement learning. This pipeline enables drones to adapt their flying speed according to the environmental complexity. Moreover, to improve the obstacle avoidance performance in cluttered environments, we propose a novel latent space. The latent space in this representation is explicitly trained to retain memory of previous depth map observations. Our findings confirm that varying speed leads to a superior balance of success rate and agility in cluttered environments. Additionally, our memory-augmented latent representation outperforms the latent representation commonly used in reinforcement learning. Finally, after minimal fine-tuning, we successfully deployed our network on a real drone for enhanced obstacle avoidance.

# 2. Installation

## 2.1 Install AvoidBench
Please refer to [AvoidBench](https://github.com/tudelft/AvoidBench) and check the dependency of installation. Run the following commands to setup:
``` bash
# install Open3D
sudo apt update
sudo apt install git libtool build-essential cmake
git clone --recursive -b v0.9.0 https://github.com/isl-org/Open3D.git
cd Open3D
mkdir build
cd build
cmake ..
make -j
sudo make install
```

``` bash
sudo apt update
sudo apt install libzmqpp-dev libopencv-dev unzip python3-catkin-tools
sudo apt install libgoogle-glog-dev protobuf-compiler ros-noetic-octomap-msgs ros-noetic-octomap-ros python3-vcstool
git clone git@github.com:tudelft/AvoidBench.git
cd AvoidBench
git checkout mavrl_version
echo "export AVOIDBENCH_PATH=path_to_this_project/AvoidBench/src/avoidbench" >> ~/.bashrc
```

## 2.1 Install MAVRL
Get the mavrl ros package:
``` bashs
cd AvoidBench/src
git clone git@github.com:tudelft/mavrl.git
```

# 3. Training
Our pipeline comprises three main components: the VAE, LSTM, and PPO. The training process is as following:
+ We begin by training a basic PPO policy, while the VAE and LSTM components are initially set to random. This foundational policy allows the drone to navigate to the target in environments without obstacles.
+ This initial policy is utilized to gather a dataset, focused primarily on capturing a multitude of depth image sequences without the concern of collisions. Subsequently, we use this dataset for the training of the VAE, bypassing the LSTM phase in this step.
+ Once the VAE is trained, we maintain the encoder in a fixed state and proceed to train the LSTM using the dataset generated by the initial policy.
+ After training both the VAE and LSTM, we freeze them and retrain the PPO, adapting it to environments of varying complexity.

Train an initial policy:
``` bash
cd AvoidBench/src/mavrl/
python train_policy --retrain 0 --train 1 --scene_id 1 # scene_id=0: indoor warehouse, scene_id=1: outdoor forest
```
We suggest to train around 200 iterations and use the last weight file as initial policy. Then use the initial policy to collect datasets for perception part:
``` bash
python collect_data.py --trial 1 --iter 200 --scene_id 1
```
where trial=1 and iter=200 means to load the weight from ```saved/RecurrentPPO_1/Policy/iter_00200.pth```. Set different ```--scene_id``` to get both indoor and outdoor data.
Train Variational AutoEncoder (VAE):
``` bash
python trainvae.py
```
Make sure you have built a folder ```exp_vae``` in ```mavrl```. Then load VAE and train LSTM:
``` bash
python train_lstm_without_env.py --trial 1 --iter 200 --recon 1 1 0 --lstm_exp LSTM_110_0
```
where trial=1 and iter=200 means to load the weight from ```saved/RecurrentPPO_1/Policy/iter_00200.pth```. The argument ```recon``` deicde if reconstruct past, current, and future depth or not. ```--recon 1 1 0``` means reconstructing past and current depth. ```lstm_exp``` defines the output folder name of LSTM training.
Load VAE and LSTM training result and retrain the policy:
```bash
python train_policy.py --retrain 1 --trial 1 --iter 1950 --scene_id 1 --nocontrol 1
```
where trial=1 and iter=1950 means to load the weight from ```saved/RecurrentPPO_1/Policy/iter_01950.pth```, make sure to change the output folder name ```LSTM_xxx_x_0``` to ```RecurrrntPPO_x``` before retrain the policy
